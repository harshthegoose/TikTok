{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **TikTok Project**\n",
    "**Course 6 - The nuts and bolts of machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ttxbfHXzB4e"
   },
   "source": [
    "Recall that you are a data professional at TikTok. Your\n",
    "supervisor was impressed with the work you have done\n",
    "and has requested that you build a machine learning model that can be used to determine whether a video contains a claim or whether it offers an opinion. With a successful prediction model, TikTok can reduce the backlog of user reports and prioritize them more efficiently.\n",
    "\n",
    "A notebook was structured and prepared to help you in this project. Please complete the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSbVJvomcVa"
   },
   "source": [
    "# **Course 6 End-of-course project: Classifying videos using machine learning**\n",
    "\n",
    "In this activity, you will practice using machine learning techniques to predict on a binary outcome variable.\n",
    "<br/>\n",
    "\n",
    "**The purpose** of this model is to mitigate misinformation in videos on the TikTok platform.\n",
    "\n",
    "**The goal** of this model is to predict whether a TikTok video presents a \"claim\" or presents an \"opinion\".\n",
    "<br/>\n",
    "\n",
    "*This activity has three parts:*\n",
    "\n",
    "**Part 1:** Ethical considerations\n",
    "* Consider the ethical implications of the request\n",
    "\n",
    "* Should the objective of the model be adjusted?\n",
    "\n",
    "**Part 2:** Feature engineering\n",
    "\n",
    "* Perform feature selection, extraction, and transformation to prepare the data for modeling\n",
    "\n",
    "**Part 3:** Modeling\n",
    "\n",
    "* Build the models, evaluate them, and advise on next steps\n",
    "\n",
    "Follow the instructions and answer the questions below to complete the activity. Then, you will complete an Executive Summary using the questions listed on the PACE Strategy Document.\n",
    "\n",
    "Be sure to complete this activity before moving on. The next course item will provide you with a completed exemplar to compare to your own work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehi9U6TkopGx"
   },
   "source": [
    "# **Classify videos using machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzDjfCSLf6Jq"
   },
   "source": [
    "<img src=\"images/Pace.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "# **PACE stages**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlVcScP0pCOJ"
   },
   "source": [
    "Throughout these project notebooks, you'll see references to the problem-solving framework PACE. The following notebook components are labeled with the respective PACE stage: Plan, Analyze, Construct, and Execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5g1A74r0ow_"
   },
   "source": [
    "<img src=\"images/Plan.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "\n",
    "## **Pace: Plan**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Plan stage.\n",
    "\n",
    "In this stage, consider the following questions:\n",
    "\n",
    "1.   **What are you being asked to do? What metric should I use to evaluate success of my business/organizational objective?**\n",
    "\n",
    "2.   **What are the ethical implications of the model? What are the consequences of your model making errors?**\n",
    "  *   What is the likely effect of the model when it predicts a false negative (i.e., when the model says a video does not contain a claim and it actually does)?\n",
    "\n",
    "  *   What is the likely effect of the model when it predicts a false positive (i.e., when the model says a video does contain a claim and it actually does not)?\n",
    "\n",
    "3.   **How would you proceed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCkZp3l0AeQn"
   },
   "source": [
    "**Exemplar responses:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjG5dmAwdbfw"
   },
   "source": [
    "**1. What are you being asked to do?**\n",
    "\n",
    "**Business need and modeling objective**\n",
    "\n",
    "TikTok users can report videos that they believe violate the platform's terms of service. Because there are millions of TikTok videos created and viewed every day, this means that many videos get reported&mdash;too many to be individually reviewed by a human moderator.\n",
    "\n",
    "Analysis indicates that when authors do violate the terms of service, they're much more likely to be presenting a claim than an opinion. Therefore, it is useful to be able to determine which videos make claims and which videos are opinions.\n",
    "\n",
    "TikTok wants to build a machine learning model to help identify claims and opinions. Videos that are labeled opinions will be less likely to go on to be reviewed by a human moderator. Videos that are labeled as claims will be further sorted by a downstream process to determine whether they should get prioritized for review. For example, perhaps videos that are classified as claims would then be ranked by how many times they were reported, then the top x% would be reviewed by a human each day.\n",
    "\n",
    "A machine learning model would greatly assist in the effort to present human moderators with videos that are most likely to be in violation of TikTok's terms of service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmZ9RNb0L-TR"
   },
   "source": [
    "**Modeling design and target variable**\n",
    "\n",
    "The data dictionary shows that there is a column called `claim_status`. This is a binary value that indicates whether a video is a claim or an opinion. This will be the target variable. In other words, for each video, the model should predict whether the video is a claim or an opinion.\n",
    "\n",
    "This is a classification task because the model is predicting a binary class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltMmCdk-2T1u"
   },
   "source": [
    "**Select an evaluation metric**\n",
    "\n",
    "To determine which evaluation metric might be best, consider how the model might be wrong. There are two possibilities for bad predictions:\n",
    "\n",
    "  - **False positives:** When the model predicts a video is a claim when in fact it is an opinion\n",
    "  - **False negatives:** When the model predicts a video is an opinion when in fact it is a claim\n",
    "\n",
    "\n",
    "\n",
    "**2. What are the ethical implications of building the model?**\n",
    "In the given scenario, it's better for the model to predict false positives when it makes a mistake, and worse for it to predict false negatives. It's very important to identify videos that break the terms of service, even if that means some opinion videos are misclassified as claims. The worst case for an opinion misclassified as a claim is that the video goes to human review. The worst case for a claim that's misclassified as an opinion is that the video does not get reviewed _and_ it violates the terms of service. A video that violates the terms of service would be considered posted from a \"banned\" author, as referenced in the data dictionary.\n",
    "\n",
    "Because it's more important to minimize false negatives, the model evaluation metric will be **recall**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H01QDrdNwF9"
   },
   "source": [
    "**3. How would you proceed?**\n",
    "\n",
    "**Modeling workflow and model selection process**\n",
    "\n",
    "Previous work with this data has revealed that there are ~20,000 videos in the sample. This is sufficient to conduct a rigorous model validation workflow, broken into the following steps:\n",
    "\n",
    "1. Split the data into train/validation/test sets (60/20/20)\n",
    "2. Fit models and tune hyperparameters on the training set\n",
    "3. Perform final model selection on the validation set\n",
    "4. Assess the champion model's performance on the test set\n",
    "\n",
    "![](https://raw.githubusercontent.com/adacert/tiktok/main/optimal_model_flow_numbered.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Vm3QEfGELS"
   },
   "source": [
    "### **Task 1. Imports and data loading**\n",
    "\n",
    "Start by importing packages needed to build machine learning models to achieve the goal of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCni9wAGphb0"
   },
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import packages for data preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, \\\n",
    "recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeXTZ2tdbALL"
   },
   "source": [
    "Load the data from the provided csv file into a dataframe.\n",
    "\n",
    "**Note:** As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "P_CZ-1P1Q62x",
    "outputId": "f960e90c-ed45-41cf-bdaa-598b1e8b4fc0"
   },
   "outputs": [],
   "source": [
    "# Load dataset into dataframe\n",
    "data = pd.read_csv(\"tiktok_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgPRBjizg1oo"
   },
   "source": [
    "<img src=\"images/Analyze.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Analyze**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Analyze stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNVl2Tb-xNB_"
   },
   "source": [
    "### **Task 2: Examine data, summary info, and descriptive stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBHl90JIRuXk"
   },
   "source": [
    "Inspect the first five rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "2rfk95MLp4a_",
    "outputId": "6fbea70a-94ef-4f1d-e9c4-822567aa296f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_duration_sec</th>\n",
       "      <th>video_transcription_text</th>\n",
       "      <th>verified_status</th>\n",
       "      <th>author_ban_status</th>\n",
       "      <th>video_view_count</th>\n",
       "      <th>video_like_count</th>\n",
       "      <th>video_share_count</th>\n",
       "      <th>video_download_count</th>\n",
       "      <th>video_comment_count</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>claim</td>\n",
       "      <td>7017666017</td>\n",
       "      <td>59</td>\n",
       "      <td>someone shared with me that drone deliveries a...</td>\n",
       "      <td>not verified</td>\n",
       "      <td>under review</td>\n",
       "      <td>343296.0</td>\n",
       "      <td>19425.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>claim</td>\n",
       "      <td>4014381136</td>\n",
       "      <td>32</td>\n",
       "      <td>someone shared with me that there are more mic...</td>\n",
       "      <td>not verified</td>\n",
       "      <td>active</td>\n",
       "      <td>140877.0</td>\n",
       "      <td>77355.0</td>\n",
       "      <td>19034.0</td>\n",
       "      <td>1161.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>claim</td>\n",
       "      <td>9859838091</td>\n",
       "      <td>31</td>\n",
       "      <td>someone shared with me that american industria...</td>\n",
       "      <td>not verified</td>\n",
       "      <td>active</td>\n",
       "      <td>902185.0</td>\n",
       "      <td>97690.0</td>\n",
       "      <td>2858.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>claim</td>\n",
       "      <td>1866847991</td>\n",
       "      <td>25</td>\n",
       "      <td>someone shared with me that the metro of st. p...</td>\n",
       "      <td>not verified</td>\n",
       "      <td>active</td>\n",
       "      <td>437506.0</td>\n",
       "      <td>239954.0</td>\n",
       "      <td>34812.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>claim</td>\n",
       "      <td>7105231098</td>\n",
       "      <td>19</td>\n",
       "      <td>someone shared with me that the number of busi...</td>\n",
       "      <td>not verified</td>\n",
       "      <td>active</td>\n",
       "      <td>56167.0</td>\n",
       "      <td>34987.0</td>\n",
       "      <td>4110.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # claim_status    video_id  video_duration_sec  \\\n",
       "0  1        claim  7017666017                  59   \n",
       "1  2        claim  4014381136                  32   \n",
       "2  3        claim  9859838091                  31   \n",
       "3  4        claim  1866847991                  25   \n",
       "4  5        claim  7105231098                  19   \n",
       "\n",
       "                            video_transcription_text verified_status  \\\n",
       "0  someone shared with me that drone deliveries a...    not verified   \n",
       "1  someone shared with me that there are more mic...    not verified   \n",
       "2  someone shared with me that american industria...    not verified   \n",
       "3  someone shared with me that the metro of st. p...    not verified   \n",
       "4  someone shared with me that the number of busi...    not verified   \n",
       "\n",
       "  author_ban_status  video_view_count  video_like_count  video_share_count  \\\n",
       "0      under review          343296.0           19425.0              241.0   \n",
       "1            active          140877.0           77355.0            19034.0   \n",
       "2            active          902185.0           97690.0             2858.0   \n",
       "3            active          437506.0          239954.0            34812.0   \n",
       "4            active           56167.0           34987.0             4110.0   \n",
       "\n",
       "   video_download_count  video_comment_count  text_length  \n",
       "0                   1.0                  0.0           97  \n",
       "1                1161.0                684.0          107  \n",
       "2                 833.0                329.0          137  \n",
       "3                1234.0                584.0          131  \n",
       "4                 547.0                152.0          128  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66KOxKCx977b"
   },
   "source": [
    "Get the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYwW-G1WqX3R",
    "outputId": "b777029a-24f3-4dfc-ef24-d5bf1f4b8b46"
   },
   "outputs": [],
   "source": [
    "# Get number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URuRVjUZ_Axg"
   },
   "source": [
    "Get basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyyKCGzCp7SS",
    "outputId": "0505065b-e64f-4fe7-f5d0-01a224b82c3f"
   },
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T5Ieb6WB61Q"
   },
   "source": [
    "Generate basic descriptive statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HbpuAS2UqY01",
    "outputId": "db89b407-30ba-405e-c4ce-5e3ccc405b57"
   },
   "outputs": [],
   "source": [
    "# Generate basic descriptive stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OpeNQDdyIT6"
   },
   "source": [
    "Check for and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrunHcfa7xnT",
    "outputId": "d958366a-c57a-4b41-d03b-b8922bbdd09b"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiadUrXRqQhg"
   },
   "source": [
    "**Exemplar response:** There are very few missing values relative to the number of samples in the dataset. Therefore, observations with missing values can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHSj1Hma914I"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcfffpANyNiu"
   },
   "source": [
    "Check for and handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKaGnWIsiHpH",
    "outputId": "a4027a67-418c-47f5-8eb8-db74c51dc294"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scLLbNSIjAWs"
   },
   "source": [
    "**Exemplar response:** There are no duplicate observations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKlzAqmoEqKN"
   },
   "source": [
    "Check for and handle outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HcHpGc5Hn7"
   },
   "source": [
    "**Exemplar response:** Tree-based models are robust to outliers, so there is no need to impute or drop any values based on where they fall in their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecg5b3Jwfyxa"
   },
   "source": [
    "Check class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQauKR11fyxb",
    "outputId": "7606a6ca-0db4-4002-ca56-84da7926a33c"
   },
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "data[\"claim_status\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYWt2p5LqlW"
   },
   "source": [
    "**Exemplar response:** Approximately 50.3% of the dataset represents claims and 49.7% represents opinions, so the outcome variable is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n1eikFh8akS"
   },
   "source": [
    "<img src=\"images/Construct.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Construct**\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Construct stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb_u1c6_T1i-"
   },
   "source": [
    "### **Task 3. Feature engineering**\n",
    "\n",
    "\n",
    "Extract the length (character count) of each `video_transcription_text` and add this to the dataframe as a new column called `text_length` so that it can be used as a feature in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "a0IaKI8dnG9h",
    "outputId": "c28076fb-a280-47e0-e925-1c90dac4a9a3"
   },
   "outputs": [],
   "source": [
    "# Create `text_length` column\n",
    "data['text_length'] = data['video_transcription_text'].str.len()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2RHONw6D3R6"
   },
   "source": [
    "Calculate the average `text_length` for claims and opinions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "nxLJEfLM5jEi",
    "outputId": "47f72289-3ee7-49b4-a4ef-9cc32ef05bf1"
   },
   "outputs": [],
   "source": [
    "data[['claim_status', 'text_length']].groupby('claim_status').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGu7ipi4AJmP"
   },
   "source": [
    "Visualize the distribution of `text_length` for claims and opinions using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MSq136S3TIYe",
    "outputId": "8c61a3f1-83ea-4d14-d874-a31ac2cc0dcf"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of `text_length` for claims and opinions\n",
    "# Create two histograms in one plot\n",
    "\n",
    "sns.histplot(data=data, stat=\"count\", multiple=\"dodge\", x=\"text_length\",\n",
    "             kde=False, palette=\"pastel\", hue=\"claim_status\",\n",
    "             element=\"bars\", legend=True)\n",
    "plt.xlabel(\"video_transcription_text length (number of characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of video_transcription_text length for claims and opinions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5acRzDOTB_7"
   },
   "source": [
    "Letter count distributions for both claims and opinions are approximately normal with a slight right skew. Claim videos tend to have more characters&mdash;about 13 more on average, as indicated in a previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZowX9rhU1o"
   },
   "source": [
    "**Feature selection and transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTCrz71_UnHS"
   },
   "source": [
    "Encode target and catgorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "DHeI3AVr309a",
    "outputId": "6f6f06ad-9453-46dd-990c-ddd688e8d319"
   },
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "# Drop unnecessary columns\n",
    "X = X.drop(['#', 'video_id'], axis=1)\n",
    "# Encode target variable\n",
    "X['claim_status'] = X['claim_status'].replace({'opinion': 0, 'claim': 1})\n",
    "# Dummy encode remaining categorical values\n",
    "X = pd.get_dummies(X,\n",
    "                   columns=['verified_status', 'author_ban_status'],\n",
    "                   drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZR2BdQxZQjN"
   },
   "source": [
    "### **Task 4. Split the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9mXFyvnWmEX"
   },
   "source": [
    "Assign target variable.\n",
    "\n",
    "**Exemplar response:**\n",
    "In this case, the target variable is `claim_status`.\n",
    "* 0 represents an opinion\n",
    "* 1 represents a claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH6jiQECBgFn"
   },
   "outputs": [],
   "source": [
    "# Isolate target variable\n",
    "y = X['claim_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPxEovpUBZfk"
   },
   "source": [
    "Isolate the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "9NYDoItCJUR-",
    "outputId": "4d3ddaec-7ffc-4df1-b15b-f712ea8e3835"
   },
   "outputs": [],
   "source": [
    "# Isolate features\n",
    "X = X.drop(['claim_status'], axis=1)\n",
    "\n",
    "# Display first few rows of features dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHOuraMm1jRy"
   },
   "source": [
    "#### **Task 5: Create train/validate/test sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tNw9_y9jmY1"
   },
   "source": [
    "Split data into training and testing sets, 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKgrew0V6o_3"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQq9zKScz-NJ"
   },
   "source": [
    "Split the training set into training and validation sets, 75/25, to result in a final ratio of 60/20/20 for train/validate/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhkqFZaJYTyK"
   },
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VlklzoujrAR"
   },
   "source": [
    "Confirm that the dimensions of the training, validation, and testing sets are in alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgbB1NCtfxcc",
    "outputId": "5458052e-8cc4-4799-fa8b-c727ec5ce9a8"
   },
   "outputs": [],
   "source": [
    "# Get shape of each training, validation, and testing set\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrPfLLbCj6WP"
   },
   "source": [
    "**Exemplar notes:**\n",
    "- The number of features (`11`) aligns between the training and testing sets.\n",
    "- The number of rows aligns between the features and the outcome variable for training (`11,450`) and both validation and testing data (`3,817`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6frX3ATWZVgL"
   },
   "source": [
    "### **BONUS CONTENT - Tokenize text column**\n",
    "\n",
    "**NOTE:** You are not expected to do this or know this, but you might find it useful and/or interesting to understand some basic ideas behind natural language processing (NLP), because of the nature of the data provided in this TikTok project.\n",
    "\n",
    "The feature `video_transcription_text` is text-based. It is not a categorical variable, since it does not have a fixed number of possible values. One way to extract numerical features from it is through a bag-of-words algorithm like [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "`CountVectorizer` works by splitting text into n-grams, which are groups of n consecutive words. For instance, \"a dime for a cup of coffee\" (phrase A) broken into 2-grams would result in six two-word combinations:\n",
    "\n",
    "`a dime` | `dime for` |`for a`| `a cup` | `cup of` | `of coffee` |\n",
    "\n",
    "Then, the next sample's text would be parsed into 2-grams. So, \"ask for a cup for a child\" (phrase B) would result in:\n",
    "\n",
    "`ask for` |`for a`| `a cup` | `cup for` | `for a` | `a child` |\n",
    "\n",
    "This process would repeat for each observation in the dataset, and each n-gram would be treated like a distinct feature. Then, the text of each observation is compared to the full array of n-grams, and the numbers of occurrences are tallied:\n",
    "\n",
    "|  | `a dime` |`dime for`  |`for a`| `a cup` | `cup of` | `of coffee` | `ask for` | `cup for` | `a child` |\n",
    "|--: |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|phrase A |1|1|1|1|1|1|0|0|0|\n",
    "|phrase B |0|0|2|1|1|0|1|1|1|\n",
    "|**TOTAL**|1|1|3|2|2|1|1|1|1|\n",
    "\n",
    "This would happen for the text of each observation in the data, and the text of each observation is parsed to get tallies for all the 2-word phrases from the entire data set for each observation, creating a large matrix.\n",
    "\n",
    "If text is broken into 1-grams, then each feature in the matrix is an individual word.\n",
    "\n",
    "After the count matrix has been created, `CountVectorizer` lets you the choose to keep only the most frequently occurring n-grams. You specify how many. The n-grams that you select can then be used as features in a model.\n",
    "\n",
    "Splitting text into n-grams is an example of tokenization. Tokenization is the process of breaking text into smaller units to derive meaning from the resulting tokens.\n",
    "\n",
    "This notebook breaks each video's transcription text into both 2-grams and 3-grams, then takes the 15 most frequently occurring tokens from the entire dataset to use as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "4r-9NHYkq8gT",
    "outputId": "93acd2c2-947a-48d8-b8d0-37e6484712d9"
   },
   "outputs": [],
   "source": [
    "# Set up a `CountVectorizer` object, which converts a collection of text to a matrix of token counts\n",
    "count_vec = CountVectorizer(ngram_range=(2, 3),\n",
    "                            max_features=15,\n",
    "                            stop_words='english')\n",
    "count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVPNQyJtNJ2V"
   },
   "source": [
    "Fit the vectorizer to the training data (generate the n-grams) and transform it (tally the occurrences). Only fit to the training data, not the validation or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4Y5K5pbrFD4",
    "outputId": "a021dca1-a9a9-4f11-8d5d-1761c2673d52"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the training set\n",
    "count_data = count_vec.fit_transform(X_train['video_transcription_text']).toarray()\n",
    "count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "LB31J6a7tm2b",
    "outputId": "9c39b5c0-1b8c-4d4b-e25e-a6c065b3a81c"
   },
   "outputs": [],
   "source": [
    "# Place the numerical representation of `video_transcription_text` from training set into a dataframe\n",
    "count_df = pd.DataFrame(data=count_data, columns=count_vec.get_feature_names_out())\n",
    "\n",
    "# Display first few rows\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "qKG1TK-KEfuB",
    "outputId": "8209e802-bd08-4be0-95a9-d21ecd23d076"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_train` and `count_df` to form the final dataframe for training data (`X_train_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_train after dropping `video_transcription_text`,\n",
    "# so that the indices align with those in `X_train` and `count_df`\n",
    "X_train_final = pd.concat([X_train.drop(columns=['video_transcription_text']).reset_index(drop=True), count_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3zgU6MZpyM7"
   },
   "source": [
    "Get n-gram counts for the validation data. Notice that the vectorizer is not being refit to the validation data. It's only transforming it. In other words, the transcriptions of the videos in the validation data are only being checked against the n-grams found in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dagpxNJup9fJ",
    "outputId": "73e12707-e35a-4343-aec5-4ae04c16735e"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the testing set\n",
    "validation_count_data = count_vec.transform(X_val['video_transcription_text']).toarray()\n",
    "validation_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "N4cC6gu5qPj2",
    "outputId": "46181cc9-32a2-4410-c4c5-c4ca5f2179c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place the numerical representation of `video_transcription_text` from validation set into a dataframe\n",
    "validation_count_df = pd.DataFrame(data=validation_count_data, columns=count_vec.get_feature_names_out())\n",
    "validation_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "S3tk8bMKqsQu",
    "outputId": "fa1c159e-ad39-440c-e06b-62184d4e3fd1"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_val` and `validation_count_df` to form the final dataframe for training data (`X_val_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_val after dropping `video_transcription_text`,\n",
    "# so that the indices align with those in `validation_count_df`\n",
    "X_val_final = pd.concat([X_val.drop(columns=['video_transcription_text']).reset_index(drop=True), validation_count_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "X_val_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMw8Gd_iqmv_"
   },
   "source": [
    "Repeat the process to get n-gram counts for the test data. Again, don't refit the vectorizer to the test data. Just transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "PPDlmol1qrMS",
    "outputId": "eea139f1-658a-4639-a7a4-b0cdb8a873e6"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the testing set\n",
    "test_count_data = count_vec.transform(X_test['video_transcription_text']).toarray()\n",
    "\n",
    "# Place the numerical representation of `video_transcription_text` from test set into a dataframe\n",
    "test_count_df = pd.DataFrame(data=test_count_data, columns=count_vec.get_feature_names_out())\n",
    "\n",
    "# Concatenate `X_val` and `validation_count_df` to form the final dataframe for training data (`X_val_final`)\n",
    "X_test_final = pd.concat([X_test.drop(columns=['video_transcription_text']\n",
    "                                      ).reset_index(drop=True), test_count_df], axis=1)\n",
    "X_test_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o16nL7LWQGfY"
   },
   "source": [
    "### **Task 6. Build models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_es-Jh1atUz"
   },
   "source": [
    "### **Build a random forest model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty8ieBkDBH4g"
   },
   "source": [
    "Fit a random forest model to the training set. Use cross-validation to tune the hyperparameters and select the model that performs best on recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAb45ShmuwLC"
   },
   "outputs": [],
   "source": [
    "# Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [5, 7, None],\n",
    "             'max_features': [0.3, 0.6],\n",
    "            #  'max_features': 'auto'\n",
    "             'max_samples': [0.7],\n",
    "             'min_samples_leaf': [1,2],\n",
    "             'min_samples_split': [2,3],\n",
    "             'n_estimators': [75,100,200],\n",
    "             }\n",
    "\n",
    "# Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "42reuVJmu8Tt",
    "outputId": "3394a80f-e031-4887-8584-47932a46eccb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_cv.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qr5k5yvwvGPC",
    "outputId": "d015e2ea-8648-44c0-e94e-52e069b2910d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948228253467271"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine best recall score\n",
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qieU0tPFvLUA",
    "outputId": "f09592e3-c2dd-4f77-cfc6-ab0bcfe028f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 0.6,\n",
       " 'max_samples': 0.7,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine best parameters\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtG--gHPOE6m"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "This model performs exceptionally well, with an average recall score of 0.995 across the five cross-validation folds. After checking the precision score to be sure the model is not classifying all samples as claims, it is clear that this model is making almost perfect classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Mf2TEWBS-hP"
   },
   "source": [
    "### **Build an XGBoost model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "E0vq2MncTLUM"
   },
   "outputs": [],
   "source": [
    "# Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [4,8,12],\n",
    "             'min_child_weight': [3, 5],\n",
    "             'learning_rate': [0.01, 0.1],\n",
    "             'n_estimators': [300, 500]\n",
    "             }\n",
    "\n",
    "# Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiSKpucwToRW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_cv.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny_s71DDTuD5"
   },
   "outputs": [],
   "source": [
    "xgb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LZEwxPJiWWRm"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "xgb_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzV00QhtQsNY"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "This model also performs exceptionally well. Although its recall score is very slightly lower than the random forest model's, its precision score is perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DasR1XS3QpB3"
   },
   "source": [
    "<img src=\"images/Execute.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Execute**\n",
    "Consider the questions in your PACE Strategy Documentto reflect on the Execute stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyepBhCTa1Yx"
   },
   "source": [
    "### **Task 7. Evaluate models**\n",
    "\n",
    "Evaluate models against validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM9egturW1eX"
   },
   "source": [
    "#### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZQbthy93bWM"
   },
   "outputs": [],
   "source": [
    "# Use the random forest \"best estimator\" model to get predictions on the validation set\n",
    "y_pred = rf_cv.best_estimator_.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNjDzuqmYU0G"
   },
   "source": [
    "Display the predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyKjLA_gYUYZ"
   },
   "outputs": [],
   "source": [
    "# Display the predictions on the validation set\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXDp4m9dYlN3"
   },
   "source": [
    "\n",
    "Display the true labels of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JymZrHVDYdvu"
   },
   "outputs": [],
   "source": [
    "# Display the true labels of the validation set\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVMG0ubSXQvS"
   },
   "source": [
    "Create a confusion matrix to visualize the results of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix to visualize the results of the classification model\n",
    "\n",
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFTNAnk9ehWp"
   },
   "source": [
    "**Exemplar notes:**\n",
    "\n",
    "The upper-left quadrant displays the number of true negatives: the number of opinions that the model accurately classified as so.\n",
    "\n",
    "The upper-right quadrant displays the number of false positives: the number of opinions that the model misclassified as claims.\n",
    "\n",
    "The lower-left quadrant displays the number of false negatives: the number of claims that the model misclassified as opinions.\n",
    "\n",
    "The lower-right quadrant displays the number of true positives: the number of claims that the model accurately classified as so.\n",
    "\n",
    "A perfect model would yield all true negatives and true positives, and no false negatives or false positives.\n",
    "\n",
    "As the above confusion matrix shows, this model does not produce any false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6OmnATfbtNd"
   },
   "source": [
    "Create a classification report that includes precision, recall, f1-score, and accuracy metrics to evaluate the performance of the model.\n",
    "<br> </br>\n",
    "\n",
    "**Note:** In other labs there was a custom-written function to extract the accuracy, precision, recall, and F<sub>1</sub> scores from the GridSearchCV report and display them in a table. You can also use scikit-learn's built-in [`classification_report()`](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) function to obtain a similar table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMxCzRa-bnUe"
   },
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "# Create classification report for random forest model\n",
    "target_labels = ['opinion', 'claim']\n",
    "print(classification_report(y_val, y_pred, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hDIbDHpcLfO"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "The classification report above shows that the random forest model scores were nearly perfect. The confusion matrix indicates that there were 10 misclassifications&mdash;five false postives and five false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeQzSiGaVBgd"
   },
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN_qN9ecdCPr"
   },
   "source": [
    "Now, evaluate the XGBoost model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kDW1frqU1gA"
   },
   "outputs": [],
   "source": [
    "#Evaluate XGBoost model\n",
    "y_pred = xgb_cv.best_estimator_.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-DOoQk8Vnk0"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zVswk-gHXLG0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute values for confusion matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m log_cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_val, \u001b[43my_pred\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create display of confusion matrix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m log_disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mlog_cm, display_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.title('XGBoost - validation set');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BIEgcNxvGu-"
   },
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "target_labels = ['opinion', 'claim']\n",
    "print(classification_report(y_val, y_pred, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw_nOfzZbUFG"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "The results of the XGBoost model were also nearly perfect. However, its errors tended to be false negatives. Identifying claims was the priority, so it's important that the model be good at capturing all actual claim videos. The random forest model has a better recall score, and is therefore the champion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-HHbB2MvYAR"
   },
   "source": [
    "### **Use champion model to predict on test data**\n",
    "\n",
    "Both random forest and XGBoost model architectures resulted in nearly perfect models. Nonetheless, in this case random forest performed a little bit better, so it is the champion model.\n",
    "\n",
    "Now, use the champion model to predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3WCYnKXvjF3"
   },
   "outputs": [],
   "source": [
    "# Use champion model to predict on test data\n",
    "y_pred = rf_cv.best_estimator_.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBxtLiVxvt9Z"
   },
   "outputs": [],
   "source": [
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.title('Random forest - test set');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLktqdiEwRY2"
   },
   "source": [
    "#### **Feature importances of champion model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C10wq1EGv0ta"
   },
   "outputs": [],
   "source": [
    "importances = rf_cv.best_estimator_.feature_importances_\n",
    "rf_importances = pd.Series(importances, index=X_test_final.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rf_importances.plot.bar(ax=ax)\n",
    "ax.set_title('Feature importances')\n",
    "ax.set_ylabel('Mean decrease in impurity')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IBN5C7rxowu"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "The most predictive features all were related to engagement levels generated by the video. This is not unexpected, as analysis from prior EDA pointed to this conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ill21hQ4ej9-"
   },
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this step use the results of the models above to formulate a conclusion. Consider the following questions:\n",
    "\n",
    "1. **Would you recommend using this model? Why or why not?**\n",
    "\n",
    "2. **What was your model doing? Can you explain how it was making predictions?**\n",
    "\n",
    "3. **Are there new features that you can engineer that might improve model performance?**\n",
    "\n",
    "4. **What features would you want to have that would likely improve the performance of your model?**\n",
    "\n",
    "Remember, sometimes your data simply will not be predictive of your chosen target. This is common. Machine learning is a powerful tool, but it is not magic. If your data does not contain predictive signal, even the most complex algorithm will not be able to deliver consistent and accurate predictions. Do not be afraid to draw this conclusion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klmW5EoPlpxe"
   },
   "source": [
    "**Exemplar response:**\n",
    "\n",
    "1. *Would you recommend using this model? Why or why not?*\n",
    "Yes, one can recommend this model because it performed well on both the validation and test holdout data. Furthermore, both precision and F<sub>1</sub> scores were consistently high. The model very successfully classified claims and opinions.\n",
    "</br>\n",
    "2. *What was your model doing? Can you explain how it was making predictions?*\n",
    "The model's most predictive features were all related to the user engagement levels associated with each video. It was classifying videos based on how many views, likes, shares, and downloads they received.\n",
    "</br>\n",
    "3. *Are there new features that you can engineer that might improve model performance?*\n",
    "Because the model currently performs nearly perfectly, there is no need to engineer any new features.\n",
    "</br>\n",
    "4. *What features would you want to have that would likely improve the performance of your model?*\n",
    "The current version of the model does not need any new features. However, it would be helpful to have the number of times the video was reported. It would also be useful to have the total number of user reports for all videos posted by each author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've completed this lab. However, you may not notice a green check mark next to this item on Coursera's platform. Please continue your progress regardless of the check mark. Just click on the \"save\" icon at the top of this notebook to ensure your work has been logged."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
